{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianellis1997/Generative_Music_Notation_and_Sound/blob/main/User_Input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parent Paper Code Run\n",
        "### Compose and Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach\n",
        "#### 17 Sep 2022  Â·  Shih-Lun Wu, Yi-Hsuan Yang\n",
        "\n",
        "### Team 9: Brian Ellis,"
      ],
      "metadata": {
        "id": "2ZE6n9mwLN2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE11mDMcqLw2",
        "outputId": "7bc18e83-1640-45ed-e82d-43ee30ed46a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Compose_and_Embellish'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 74 (delta 28), reused 38 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 52.25 KiB | 3.73 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Cloning into 'Generative_Music_Notation_and_Sound'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "Receiving objects: 100% (36/36), 23.38 KiB | 7.79 MiB/s, done.\n",
            "remote: Total 36 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/cifkao/fast-transformers.git@39e726864d1a279c9719d33a95868a4ea2fb5ac5\n",
            "  Cloning https://github.com/cifkao/fast-transformers.git (to revision 39e726864d1a279c9719d33a95868a4ea2fb5ac5) to /tmp/pip-req-build-nvsslz06\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cifkao/fast-transformers.git /tmp/pip-req-build-nvsslz06\n",
            "  Running command git rev-parse -q --verify 'sha^39e726864d1a279c9719d33a95868a4ea2fb5ac5'\n",
            "  Running command git fetch -q https://github.com/cifkao/fast-transformers.git 39e726864d1a279c9719d33a95868a4ea2fb5ac5\n",
            "  Running command git checkout -q 39e726864d1a279c9719d33a95868a4ea2fb5ac5\n",
            "  Resolved https://github.com/cifkao/fast-transformers.git to commit 39e726864d1a279c9719d33a95868a4ea2fb5ac5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-fast-transformers==0.3.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch-fast-transformers==0.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-fast-transformers==0.3.0) (1.3.0)\n",
            "Building wheels for collected packages: pytorch-fast-transformers\n"
          ]
        }
      ],
      "source": [
        "# Clone repositories\n",
        "!git clone https://github.com/slSeanWU/Compose_and_Embellish\n",
        "!git clone https://github.com/brianellis1997/Generative_Music_Notation_and_Sound.git\n",
        "\n",
        "# Install libraries\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install pre-trained transformers (15 min runtime)\n",
        "!pip install git+https://github.com/cifkao/fast-transformers.git@39e726864d1a279c9719d33a95868a4ea2fb5ac5\n",
        "!git clone https://huggingface.co/slseanwu/compose-and-embellish-pop1k7\n",
        "!pip install miditoolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "FtemWO1LpPYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ],
      "metadata": {
        "id": "aqUh40wgqL7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_causal_mask(seq_len, device):\n",
        "    mask = (torch.triu(torch.ones(seq_len, seq_len, device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    mask.requires_grad = False\n",
        "    return mask\n",
        "\n",
        "def generate_bidirectional_pad_mask(max_seqlen, batch_lens):\n",
        "    mask = torch.zeros(len(batch_lens), max_seqlen, dtype=bool)\n",
        "    for i, l in enumerate(batch_lens):\n",
        "        mask[i, l:] = True\n",
        "    return mask\n",
        "\n",
        "def weight_init_normal(weight, normal_std):\n",
        "  nn.init.normal_(weight, 0.0, normal_std)\n",
        "\n",
        "def weight_init_orthogonal(weight, gain):\n",
        "  nn.init.orthogonal_(weight, gain)\n",
        "\n",
        "def bias_init(bias):\n",
        "  nn.init.constant_(bias, 0.0)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    # print ('[{}] initializing ...'.format(classname))\n",
        "\n",
        "    if classname.find('Linear') != -1:\n",
        "        if hasattr(m, 'weight') and m.weight is not None:\n",
        "            weight_init_normal(m.weight, 0.01)\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            bias_init(m.bias)\n",
        "    elif classname.find('Embedding') != -1:\n",
        "        if hasattr(m, 'weight'):\n",
        "            weight_init_normal(m.weight, 0.01)\n",
        "    elif classname.find('LayerNorm') != -1:\n",
        "        if hasattr(m, 'weight'):\n",
        "            nn.init.normal_(m.weight, 1.0, 0.01)\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            bias_init(m.bias)\n",
        "    elif classname.find('ProjectedAdaptiveLogSoftmax') != -1:\n",
        "        if hasattr(m, 'cluster_weight') and m.cluster_weight is not None:\n",
        "            weight_init_normal(m.cluster_weight, 0.01)\n",
        "        if hasattr(m, 'cluster_bias') and m.cluster_bias is not None:\n",
        "            bias_init(m.cluster_bias)\n",
        "        if hasattr(m, 'out_projs'):\n",
        "            for i in range(len(m.out_projs)):\n",
        "                if m.out_projs[i] is not None:\n",
        "                    weight_init_normal(m.out_projs[i], 0.02)\n",
        "    elif classname.find('TXLDecoder') != -1:\n",
        "        if hasattr(m, 'r_emb'):\n",
        "            weight_init_normal(m.r_emb, 0.01)\n",
        "        if hasattr(m, 'r_w_bias'):\n",
        "            weight_init_normal(m.r_w_bias, 0.01)\n",
        "        if hasattr(m, 'r_r_bias'):\n",
        "            weight_init_normal(m.r_r_bias, 0.01)\n",
        "        if hasattr(m, 'r_bias'):\n",
        "            bias_init(m.r_bias)\n",
        "    elif classname.find('LSTM') != -1:\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:  # weights\n",
        "                weight_init_orthogonal(param, 0.01)\n",
        "            else:                      # biases\n",
        "                bias_init(param)\n",
        "    # else:\n",
        "    #   print ('*** [ {:64} ] not initialized !!'.format(classname))\n",
        "\n",
        "\n",
        "class SinusoidalPE(nn.Module):\n",
        "    def __init__(self, d_embed, max_pos=20480):\n",
        "        super(SinusoidalPE, self).__init__()\n",
        "        self.d_embed = d_embed\n",
        "        self.max_pos = max_pos\n",
        "\n",
        "        pe = torch.zeros(max_pos, d_embed)\n",
        "        position = torch.arange(0, max_pos, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_embed, 2).float() * (-math.log(10000.0) / d_embed))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, seq_len, bsz=None):\n",
        "        pos_encoding = self.pe[:seq_len, :]\n",
        "\n",
        "        if bsz is not None:\n",
        "          pos_encoding = pos_encoding.expand(seq_len, bsz, -1)\n",
        "\n",
        "        return pos_encoding\n",
        "\n",
        "class WordEmbedding(nn.Module):\n",
        "    def __init__(self, n_token, d_embed, d_proj, emb_scale=0.5, pad_idx=None):\n",
        "        super(WordEmbedding, self).__init__()\n",
        "\n",
        "        self.n_token = n_token\n",
        "        self.d_embed = d_embed\n",
        "        self.d_proj = d_proj\n",
        "        self.emb_scale = d_proj ** emb_scale\n",
        "\n",
        "        if pad_idx is None:\n",
        "            pad_idx = n_token - 1\n",
        "\n",
        "        self.emb_lookup = nn.Embedding(n_token, d_embed, padding_idx=pad_idx)\n",
        "        if d_proj != d_embed:\n",
        "            self.emb_proj = nn.Linear(d_embed, d_proj, bias=False)\n",
        "        else:\n",
        "            self.emb_proj = None\n",
        "\n",
        "    def forward(self, inp_tokens):\n",
        "        inp_emb = self.emb_lookup(inp_tokens)\n",
        "\n",
        "        if self.emb_proj is not None:\n",
        "            inp_emb = self.emb_proj(inp_emb)\n",
        "\n",
        "        return inp_emb.mul_(self.emb_scale)\n",
        "\n",
        "class OctaveAwarePitchEmbedding(nn.Module):\n",
        "    def __init__(self, n_octave, d_embed, d_proj, idx2event,\n",
        "                 emb_scale=0.5, n_chroma=12, min_pitch=12\n",
        "        ):\n",
        "        super(OctaveAwarePitchEmbedding, self).__init__()\n",
        "\n",
        "        self.n_octave = n_octave\n",
        "        self.n_chroma = n_chroma\n",
        "        self.min_pitch = min_pitch\n",
        "\n",
        "        self.d_embed = d_embed\n",
        "        self.d_proj = d_proj\n",
        "        self.emb_scale = d_proj ** emb_scale\n",
        "\n",
        "        self.octave_emb_lookup = nn.Embedding(\n",
        "            n_octave + 1, d_embed // 2, padding_idx=n_octave\n",
        "        )\n",
        "        self.chroma_emb_lookup = nn.Embedding(\n",
        "            n_chroma + 1, d_embed // 2, padding_idx=n_chroma\n",
        "        )\n",
        "\n",
        "        if d_proj != d_embed:\n",
        "            self.emb_proj = nn.Linear(d_embed, d_proj, bias=False)\n",
        "        else:\n",
        "            self.emb_proj = None\n",
        "\n",
        "        self.octave_translate_dict, self.chroma_translate_dict =\\\n",
        "            self.make_idx_translate_dicts(idx2event)\n",
        "\n",
        "    def make_idx_translate_dicts(self, idx2event):\n",
        "        idx2event[ len(idx2event) ] = 'PAD_None'\n",
        "\n",
        "        octave_dict = dict()\n",
        "        chroma_dict = dict()\n",
        "        for idx, ev in idx2event.items():\n",
        "            if not 'Note_Pitch' in ev:\n",
        "                octave_dict[idx] = self.n_octave\n",
        "                chroma_dict[idx] = self.n_chroma\n",
        "            else:\n",
        "                pitch = int(ev.split('_')[-1])\n",
        "                pitch -= self.min_pitch\n",
        "                octave_dict[idx] = pitch // self.n_chroma\n",
        "                chroma_dict[idx] = pitch % self.n_chroma\n",
        "\n",
        "        return octave_dict, chroma_dict\n",
        "\n",
        "    def forward(self, inp_tokens):\n",
        "        # st = time.time()\n",
        "        orig_device = inp_tokens.device\n",
        "\n",
        "        octave_tokens = inp_tokens.clone().cpu()\n",
        "        chroma_tokens = inp_tokens.clone().cpu()\n",
        "\n",
        "        octave_tokens.apply_(self.octave_translate_dict.get)\n",
        "        chroma_tokens.apply_(self.chroma_translate_dict.get)\n",
        "        octave_tokens = octave_tokens.to(orig_device)\n",
        "        chroma_tokens = chroma_tokens.to(orig_device)\n",
        "        # print ('[mapping] {:.3f}'.format(time.time() - st))\n",
        "\n",
        "        # st = time.time()\n",
        "        octave_emb = self.octave_emb_lookup(octave_tokens)\n",
        "        chroma_emb = self.chroma_emb_lookup(chroma_tokens)\n",
        "        inp_emb = torch.cat([octave_emb, chroma_emb], dim=-1)\n",
        "\n",
        "        # print ('[bedding] {:.3f}'.format(time.time() - st))\n",
        "\n",
        "        if self.emb_proj is not None:\n",
        "            inp_emb = self.emb_proj(inp_emb)\n",
        "\n",
        "        return inp_emb.mul_(self.emb_scale)\n",
        "\n",
        "def get_min_max_pitch_idx(idx2event):\n",
        "    min_idx, max_idx = len(idx2event), 0\n",
        "\n",
        "    for k, v in idx2event.items():\n",
        "        if 'Note_Pitch' in v:\n",
        "            min_idx = min(min_idx, k)\n",
        "            max_idx = max(max_idx, k)\n",
        "\n",
        "    return min_idx, max_idx"
      ],
      "metadata": {
        "id": "4nGD6mCQqQE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "bSeo-nxGpzoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from .transformer_helpers import WordEmbedding\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, demb):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        self.demb = demb\n",
        "\n",
        "        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, pos_seq, bsz=None):\n",
        "        sinusoid_inp = torch.ger(pos_seq, self.inv_freq)\n",
        "        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n",
        "\n",
        "        if bsz is not None:\n",
        "            return pos_emb[:,None,:].expand(-1, bsz, -1)\n",
        "        else:\n",
        "            return pos_emb[:,None,:]\n",
        "\n",
        "\n",
        "\n",
        "class PositionwiseFF(nn.Module):\n",
        "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False):\n",
        "        super(PositionwiseFF, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_inner = d_inner\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.CoreNet = nn.Sequential(\n",
        "            nn.Linear(d_model, d_inner), nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_inner, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if self.pre_lnorm:\n",
        "            ##### layer normalization + positionwise feed-forward\n",
        "            core_out = self.CoreNet(self.layer_norm(inp))\n",
        "\n",
        "            ##### residual connection\n",
        "            output = core_out + inp\n",
        "        else:\n",
        "            ##### positionwise feed-forward\n",
        "            core_out = self.CoreNet(inp)\n",
        "\n",
        "            ##### residual connection + layer normalization\n",
        "            output = self.layer_norm(inp + core_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttn(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n",
        "                 pre_lnorm=False, **kwargs):\n",
        "        super(MultiHeadCrossAttn, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n",
        "        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.dropatt = nn.Dropout(dropatt)\n",
        "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.scale = 1 / (d_head ** 0.5)\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "    def forward(self, h, c, attn_mask=None, h_pos_embed=None, c_pos_embed=None):\n",
        "        if self.pre_lnorm:\n",
        "            ##### layer normalization\n",
        "            c = self.layer_norm(c)\n",
        "\n",
        "        if h_pos_embed is not None:\n",
        "            h_ = h + self.drop(h_pos_embed)\n",
        "        else:\n",
        "            h_ = h\n",
        "        if c_pos_embed is not None:\n",
        "            c_ = c + self.drop(c_pos_embed)\n",
        "        else:\n",
        "            c_ = c\n",
        "\n",
        "        head_q = self.q_net(h_)\n",
        "        head_k, head_v = torch.chunk(self.kv_net(c_), 2, -1)\n",
        "\n",
        "        head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head)\n",
        "        head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head)\n",
        "        head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head)\n",
        "\n",
        "        # print ('[cross inputs]', head_q.mean(), head_k.mean(), head_v.mean())\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_score = torch.einsum('ibnd,jbnd->ijbn', (head_q, head_k))\n",
        "        attn_score.mul_(self.scale)\n",
        "        # print ('[attn score]', attn_score.mean(), attn_score.std())\n",
        "        if attn_mask is not None and attn_mask.any().item():\n",
        "            if attn_mask.dim() == 2:\n",
        "                attn_score.masked_fill_(attn_mask[None,:,:,None], -float('inf'))\n",
        "            elif attn_mask.dim() == 3:\n",
        "                attn_score.masked_fill_(attn_mask[:,:,:,None], -float('inf'))\n",
        "        # print (attn_score[0, :128, :, 0])\n",
        "        # print ('[masked attn score]', attn_score.mean(), attn_score.std())\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_prob = F.softmax(attn_score, dim=1)\n",
        "        attn_prob = self.dropatt(attn_prob)\n",
        "        attn_prob = attn_prob / (torch.sum(attn_prob, dim=1)[:, None, :, :] + 1e-8)\n",
        "        # print (attn_prob[0, :128, :, 0])\n",
        "        # print (torch.isnan(attn_prob).sum())\n",
        "        # idx = torch.nonzero(torch.isnan(attn_prob))\n",
        "        # for i in idx:\n",
        "        #     print (i)\n",
        "        # print ('[cross attn prob]', attn_prob.mean(), attn_prob.std())\n",
        "        # exit()\n",
        "\n",
        "        # [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]\n",
        "        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, head_v))\n",
        "        attn_vec = attn_vec.contiguous().view(\n",
        "            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n",
        "        # print ('[cross attn vec]', attn_vec.mean(), attn_vec.std())\n",
        "\n",
        "        ##### linear projection\n",
        "        attn_out = self.o_net(attn_vec)\n",
        "        attn_out = self.drop(attn_out)\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            ##### residual connection\n",
        "            output = h + attn_out\n",
        "        else:\n",
        "            ##### residual connection + layer normalization\n",
        "            output = self.layer_norm(h + attn_out)\n",
        "        # print ('[cross attn out]', attn_out.mean(), attn_out.std())\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MultiHeadAttn(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n",
        "                 pre_lnorm=False):\n",
        "        super(MultiHeadAttn, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.q_net = nn.Linear(d_model, n_head * d_head, bias=False)\n",
        "        self.kv_net = nn.Linear(d_model, 2 * n_head * d_head, bias=False)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.dropatt = nn.Dropout(dropatt)\n",
        "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.scale = 1 / (d_head ** 0.5)\n",
        "\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "    def forward(self, h, attn_mask=None, mems=None):\n",
        "        ##### multihead attention\n",
        "        # [hlen x bsz x n_head x d_head]\n",
        "\n",
        "        if mems is not None:\n",
        "            c = torch.cat([mems, h], 0)\n",
        "        else:\n",
        "            c = h\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            ##### layer normalization\n",
        "            c = self.layer_norm(c)\n",
        "\n",
        "        head_q = self.q_net(h)\n",
        "        head_k, head_v = torch.chunk(self.kv_net(c), 2, -1)\n",
        "\n",
        "        head_q = head_q.view(h.size(0), h.size(1), self.n_head, self.d_head)\n",
        "        head_k = head_k.view(c.size(0), c.size(1), self.n_head, self.d_head)\n",
        "        head_v = head_v.view(c.size(0), c.size(1), self.n_head, self.d_head)\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_score = torch.einsum('ibnd,jbnd->ijbn', (head_q, head_k))\n",
        "        attn_score.mul_(self.scale)\n",
        "        if attn_mask is not None and attn_mask.any().item():\n",
        "            if attn_mask.dim() == 2:\n",
        "                attn_score.masked_fill_(attn_mask[None,:,:,None], -float('inf'))\n",
        "            elif attn_mask.dim() == 3:\n",
        "                attn_score.masked_fill_(attn_mask[:,:,:,None], -float('inf'))\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_prob = F.softmax(attn_score, dim=1)\n",
        "        attn_prob = self.dropatt(attn_prob)\n",
        "        # attn_prob = attn_prob / (torch.sum(attn_prob, dim=1)[:, None, :, :] + 1e-8)\n",
        "\n",
        "        # [qlen x klen x bsz x n_head] + [klen x bsz x n_head x d_head] -> [qlen x bsz x n_head x d_head]\n",
        "        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, head_v))\n",
        "        attn_vec = attn_vec.contiguous().view(\n",
        "            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n",
        "\n",
        "        ##### linear projection\n",
        "        attn_out = self.o_net(attn_vec)\n",
        "        attn_out = self.drop(attn_out)\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            ##### residual connection\n",
        "            output = h + attn_out\n",
        "        else:\n",
        "            ##### residual connection + layer normalization\n",
        "            output = self.layer_norm(h + attn_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class RelMultiHeadAttn(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n",
        "                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False, **kwargs):\n",
        "        super(RelMultiHeadAttn, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.dropatt = nn.Dropout(dropatt)\n",
        "        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.scale = 1 / (d_head ** 0.5)\n",
        "\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "    def _parallelogram_mask(self, h, w, left=False):\n",
        "        mask = torch.ones((h, w)).byte()\n",
        "        m = min(h, w)\n",
        "        mask[:m,:m] = torch.triu(mask[:m,:m])\n",
        "        mask[-m:,-m:] = torch.tril(mask[-m:,-m:])\n",
        "\n",
        "        if left:\n",
        "            return mask\n",
        "        else:\n",
        "            return mask.flip(0)\n",
        "\n",
        "    def _shift(self, x, qlen, klen, mask, left=False):\n",
        "        if qlen > 1:\n",
        "            zero_pad = torch.zeros((x.size(0), qlen-1, x.size(2), x.size(3)),\n",
        "                                    device=x.device, dtype=x.dtype)\n",
        "        else:\n",
        "            zero_pad = torch.zeros(0, device=x.device, dtype=x.dtype)\n",
        "\n",
        "        if left:\n",
        "            mask = mask.flip(1)\n",
        "            x_padded = torch.cat([zero_pad, x], dim=1).expand(qlen, -1, -1, -1)\n",
        "        else:\n",
        "            x_padded = torch.cat([x, zero_pad], dim=1).expand(qlen, -1, -1, -1)\n",
        "\n",
        "        x = x_padded.masked_select(mask[:,:,None,None]) \\\n",
        "                    .view(qlen, klen, x.size(2), x.size(3))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _rel_shift(self, x, zero_triu=False):\n",
        "        zero_pad = torch.zeros((x.size(0), 1, *x.size()[2:]),\n",
        "                               device=x.device, dtype=x.dtype)\n",
        "        x_padded = torch.cat([zero_pad, x], dim=1)\n",
        "\n",
        "        x_padded = x_padded.view(x.size(1) + 1, x.size(0), *x.size()[2:])\n",
        "\n",
        "        x = x_padded[1:].view_as(x)\n",
        "\n",
        "        if zero_triu:\n",
        "            ones = torch.ones((x.size(0), x.size(1)))\n",
        "            x = x * torch.tril(ones, x.size(1) - x.size(0))[:,:,None,None]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, w, r, attn_mask=None, mems=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class RelPartialLearnableMultiHeadAttn(RelMultiHeadAttn):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(RelPartialLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n",
        "\n",
        "    def forward(self, w, r, r_w_bias, r_r_bias, attn_mask=None, mems=None, return_avg_attn=False):\n",
        "        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n",
        "\n",
        "        if mems is not None:\n",
        "            cat = torch.cat([mems, w], 0)\n",
        "            if self.pre_lnorm:\n",
        "                w_heads = self.qkv_net(self.layer_norm(cat))\n",
        "            else:\n",
        "                w_heads = self.qkv_net(cat)\n",
        "            r_head_k = self.r_net(r)\n",
        "\n",
        "            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
        "            w_head_q = w_head_q[-qlen:]\n",
        "        else:\n",
        "            if self.pre_lnorm:\n",
        "                w_heads = self.qkv_net(self.layer_norm(w))\n",
        "            else:\n",
        "                w_heads = self.qkv_net(w)\n",
        "            r_head_k = self.r_net(r)\n",
        "\n",
        "            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
        "\n",
        "        klen = w_head_k.size(0)\n",
        "\n",
        "        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
        "        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
        "        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n",
        "\n",
        "        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n",
        "\n",
        "        #### compute attention score\n",
        "        rw_head_q = w_head_q + r_w_bias                                         # qlen x bsz x n_head x d_head\n",
        "        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n",
        "\n",
        "        rr_head_q = w_head_q + r_r_bias\n",
        "        BD = torch.einsum('ibnd,jnd->ijbn', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n",
        "        BD = self._rel_shift(BD)\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_score = AC + BD\n",
        "        attn_score.mul_(self.scale)\n",
        "\n",
        "        #### compute attention probability\n",
        "        if attn_mask is not None and attn_mask.any().item():\n",
        "            if attn_mask.dim() == 2:\n",
        "                attn_score = attn_score.float().masked_fill(\n",
        "                    attn_mask[None,:,:,None], -float('inf')).type_as(attn_score)\n",
        "            elif attn_mask.dim() == 3:\n",
        "                attn_score = attn_score.float().masked_fill(\n",
        "                    attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)\n",
        "        # print ('[masked self attn score]', attn_score.mean(), attn_score.std())\n",
        "\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_prob = F.softmax(attn_score, dim=1)\n",
        "        if return_avg_attn:\n",
        "            avg_attn_prob = attn_prob.mean(dim=-1)\n",
        "        attn_prob = self.dropatt(attn_prob)\n",
        "        attn_prob = attn_prob / (torch.sum(attn_prob, dim=1)[:, None, :, :] + 1e-8)\n",
        "        # print ('[self attn prob]', attn_prob.mean(), attn_prob.std())\n",
        "\n",
        "        #### compute attention vector\n",
        "        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n",
        "\n",
        "        # [qlen x bsz x n_head x d_head]\n",
        "        attn_vec = attn_vec.contiguous().view(\n",
        "            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n",
        "\n",
        "        ##### linear projection\n",
        "        attn_out = self.o_net(attn_vec)\n",
        "        attn_out = self.drop(attn_out)\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            ##### residual connection\n",
        "            output = w + attn_out\n",
        "        else:\n",
        "            ##### residual connection + layer normalization\n",
        "            output = self.layer_norm(w + attn_out)\n",
        "\n",
        "        if not return_avg_attn:\n",
        "            return output\n",
        "        else:\n",
        "            return output, avg_attn_prob\n",
        "\n",
        "\n",
        "class RelLearnableMultiHeadAttn(RelMultiHeadAttn):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(RelLearnableMultiHeadAttn, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, w, r_emb, r_w_bias, r_bias, attn_mask=None, mems=None):\n",
        "        # r_emb: [klen, n_head, d_head], used for term B\n",
        "        # r_w_bias: [n_head, d_head], used for term C\n",
        "        # r_bias: [klen, n_head], used for term D\n",
        "\n",
        "        qlen, bsz = w.size(0), w.size(1)\n",
        "\n",
        "        if mems is not None:\n",
        "            cat = torch.cat([mems, w], 0)\n",
        "            if self.pre_lnorm:\n",
        "                w_heads = self.qkv_net(self.layer_norm(cat))\n",
        "            else:\n",
        "                w_heads = self.qkv_net(cat)\n",
        "            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
        "\n",
        "            w_head_q = w_head_q[-qlen:]\n",
        "        else:\n",
        "            if self.pre_lnorm:\n",
        "                w_heads = self.qkv_net(self.layer_norm(w))\n",
        "            else:\n",
        "                w_heads = self.qkv_net(w)\n",
        "            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n",
        "\n",
        "        klen = w_head_k.size(0)\n",
        "\n",
        "        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)\n",
        "        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)\n",
        "        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)\n",
        "\n",
        "        if klen > r_emb.size(0):\n",
        "            r_emb_pad = r_emb[0:1].expand(klen-r_emb.size(0), -1, -1)\n",
        "            r_emb = torch.cat([r_emb_pad, r_emb], 0)\n",
        "            r_bias_pad = r_bias[0:1].expand(klen-r_bias.size(0), -1)\n",
        "            r_bias = torch.cat([r_bias_pad, r_bias], 0)\n",
        "        else:\n",
        "            r_emb = r_emb[-klen:]\n",
        "            r_bias = r_bias[-klen:]\n",
        "\n",
        "        #### compute attention score\n",
        "        rw_head_q = w_head_q + r_w_bias[None]                                   # qlen x bsz x n_head x d_head\n",
        "\n",
        "        AC = torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n",
        "        B_ = torch.einsum('ibnd,jnd->ijbn', (w_head_q, r_emb))                  # qlen x klen x bsz x n_head\n",
        "        D_ = r_bias[None, :, None]                                              # 1    x klen x 1   x n_head\n",
        "        BD = self._rel_shift(B_ + D_)\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_score = AC + BD\n",
        "        attn_score.mul_(self.scale)\n",
        "\n",
        "        #### compute attention probability\n",
        "        if attn_mask is not None and attn_mask.any().item():\n",
        "            if attn_mask.dim() == 2:\n",
        "                attn_score.masked_fill_(attn_mask[None,:,:,None], -float('inf'))\n",
        "            elif attn_mask.dim() == 3:\n",
        "                attn_score.masked_fill_(attn_mask[:,:,:,None], -float('inf'))\n",
        "\n",
        "        # [qlen x klen x bsz x n_head]\n",
        "        attn_prob = F.softmax(attn_score, dim=1)\n",
        "        attn_prob = self.dropatt(attn_prob)\n",
        "        attn_prob = attn_prob / (torch.sum(attn_prob, dim=1)[:, None, :, :] + 1e-8)\n",
        "\n",
        "        #### compute attention vector\n",
        "        attn_vec = torch.einsum('ijbn,jbnd->ibnd', (attn_prob, w_head_v))\n",
        "\n",
        "        # [qlen x bsz x n_head x d_head]\n",
        "        attn_vec = attn_vec.contiguous().view(\n",
        "            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n",
        "\n",
        "        ##### linear projection\n",
        "        attn_out = self.o_net(attn_vec)\n",
        "        attn_out = self.drop(attn_out)\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            ##### residual connection\n",
        "            output = w + attn_out\n",
        "        else:\n",
        "            ##### residual connection + layer normalization\n",
        "            output = self.layer_norm(w + attn_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_head, d_inner, dropout, **kwargs):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_attn = MultiHeadAttn(n_head, d_model, d_head, dropout, **kwargs)\n",
        "        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout,\n",
        "                                     pre_lnorm=kwargs.get('pre_lnorm'))\n",
        "\n",
        "    def forward(self, dec_inp, dec_attn_mask=None, mems=None):\n",
        "\n",
        "        output = self.dec_attn(dec_inp, attn_mask=dec_attn_mask,\n",
        "                               mems=mems)\n",
        "        output = self.pos_ff(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class RelLearnableDecoderLayer(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n",
        "                 **kwargs):\n",
        "        super(RelLearnableDecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_attn = RelLearnableMultiHeadAttn(n_head, d_model, d_head, dropout,\n",
        "                                         **kwargs)\n",
        "        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout,\n",
        "                                     pre_lnorm=kwargs.get('pre_lnorm'))\n",
        "\n",
        "    def forward(self, dec_inp, r_emb, r_w_bias, r_bias, dec_attn_mask=None, mems=None):\n",
        "\n",
        "        output = self.dec_attn(dec_inp, r_emb, r_w_bias, r_bias,\n",
        "                               attn_mask=dec_attn_mask,\n",
        "                               mems=mems)\n",
        "        output = self.pos_ff(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class RelPartialLearnableDecoderLayer(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_head, d_inner, dropout,\n",
        "                 **kwargs):\n",
        "        super(RelPartialLearnableDecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n",
        "                            d_head, dropout, **kwargs)\n",
        "        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout,\n",
        "                                     pre_lnorm=kwargs.get('pre_lnorm'))\n",
        "\n",
        "        if 'use_cross_attn' in kwargs and kwargs.get('use_cross_attn') is True:\n",
        "            self.cross_attn = MultiHeadCrossAttn(n_head, d_model, d_head, dropout, **kwargs)\n",
        "        else:\n",
        "            self.cross_attn = None\n",
        "\n",
        "    def forward(self, dec_inp, r, r_w_bias, r_r_bias, dec_attn_mask=None, mems=None,\n",
        "                cross_latent=None, dec_cross_pos_emb=None, latent_cross_pos_emb=None,\n",
        "                cross_attn_mask=None, return_avg_attn=False):\n",
        "\n",
        "        if not return_avg_attn:\n",
        "            output = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,\n",
        "                                attn_mask=dec_attn_mask,\n",
        "                                mems=mems, return_avg_attn=False)\n",
        "        else:\n",
        "            output, avg_attn_prob = self.dec_attn(dec_inp, r, r_w_bias, r_r_bias,\n",
        "                                                  attn_mask=dec_attn_mask,\n",
        "                                                  mems=mems, return_avg_attn=True)\n",
        "\n",
        "        if self.cross_attn is not None and cross_latent is not None:\n",
        "            if dec_cross_pos_emb is None:\n",
        "                dec_cross_pos_emb = torch.zeros_like(dec_inp)\n",
        "            if latent_cross_pos_emb is None:\n",
        "                latent_cross_pos_emb = torch.zeros_like(cross_latent)\n",
        "\n",
        "            output = self.cross_attn.forward(\n",
        "                                output, cross_latent,\n",
        "                                attn_mask=cross_attn_mask,\n",
        "                                h_pos_embed=dec_cross_pos_emb,\n",
        "                                c_pos_embed=latent_cross_pos_emb\n",
        "                            )\n",
        "\n",
        "        output = self.pos_ff(output)\n",
        "\n",
        "        if not return_avg_attn:\n",
        "            return output\n",
        "        else:\n",
        "            return output, avg_attn_prob\n",
        "\n",
        "class SegmentEmbeddingProj(nn.Module):\n",
        "    def __init__(self, d_in, d_out, n_layer=None, tie_seg_emb_projs=True, scale=1.):\n",
        "        super(SegmentEmbeddingProj, self).__init__()\n",
        "        self.d_in = d_in\n",
        "        self.d_out = d_out\n",
        "\n",
        "        self.emb_proj = nn.ModuleList()\n",
        "        self.tie_seg_emb_projs = tie_seg_emb_projs\n",
        "\n",
        "        if tie_seg_emb_projs:\n",
        "            self.emb_proj.append( nn.Linear(d_in, d_out, bias=False) )\n",
        "        else:\n",
        "            for l in range(n_layer):\n",
        "                self.emb_proj.append( nn.Linear(d_in, d_out, bias=False) )\n",
        "\n",
        "        self.scale = scale\n",
        "        print ('[seg emb scale]', scale)\n",
        "\n",
        "    def forward(self, inp, layer=None):\n",
        "        if layer is None or self.tie_seg_emb_projs:\n",
        "            emb_out = self.emb_proj[0](inp)\n",
        "        else:\n",
        "            emb_out = self.emb_proj[layer](inp)\n",
        "\n",
        "        return emb_out.mul_(self.scale)\n",
        "\n",
        "\n",
        "class OptimusTXLDecoder(nn.Module):\n",
        "    def __init__(self, n_layer, n_head, d_model, d_head, d_inner, d_segment_emb,\n",
        "                 dropout, dropatt, pre_lnorm=False, use_segment_emb=True,\n",
        "                 tgt_len=None, ext_len=None, mem_len=None,\n",
        "                 same_length=False, attn_type=0, clamp_len=-1,\n",
        "                 tie_seg_emb_projs=True, in_attn_cond=True,\n",
        "                 use_cross_attn=False, cross_len=192, seg_proj_scale=1.\n",
        "        ):\n",
        "        super(OptimusTXLDecoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_head = n_head\n",
        "        self.d_head = d_head\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.n_layer = n_layer\n",
        "        self.d_segment_emb = d_segment_emb\n",
        "\n",
        "        self.tgt_len = tgt_len\n",
        "        self.mem_len = mem_len\n",
        "        self.ext_len = ext_len\n",
        "        self.max_klen = tgt_len + ext_len + mem_len\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "        self.use_segment_emb = use_segment_emb\n",
        "\n",
        "        self.tie_seg_emb_projs = tie_seg_emb_projs\n",
        "        self.in_attn_cond = in_attn_cond\n",
        "\n",
        "        if self.use_segment_emb:\n",
        "            self.seg_proj_scale = seg_proj_scale\n",
        "            self.seg_emb_projs = SegmentEmbeddingProj(\n",
        "                                    d_segment_emb, d_model, n_layer, tie_seg_emb_projs,\n",
        "                                    scale=self.seg_proj_scale\n",
        "                                )\n",
        "        else:\n",
        "            self.seg_emb_projs = None\n",
        "\n",
        "        self.use_cross_attn = use_cross_attn\n",
        "        if self.use_cross_attn:\n",
        "            self.cross_len = cross_len\n",
        "            self.cross_pos_emb = WordEmbedding(\n",
        "                                    cross_len, d_model, d_model, emb_scale=0.2\n",
        "                                )\n",
        "\n",
        "        self.attn_type = attn_type\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        if attn_type == 0: # the default attention\n",
        "            for i in range(n_layer):\n",
        "                self.layers.append(\n",
        "                    RelPartialLearnableDecoderLayer(\n",
        "                        n_head, d_model, d_head, d_inner, dropout,\n",
        "                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n",
        "                        dropatt=dropatt, pre_lnorm=pre_lnorm,\n",
        "                        use_cross_attn=use_cross_attn)\n",
        "                )\n",
        "        elif attn_type == 1: # learnable embeddings\n",
        "            for i in range(n_layer):\n",
        "                self.layers.append(\n",
        "                    RelLearnableDecoderLayer(\n",
        "                        n_head, d_model, d_head, d_inner, dropout,\n",
        "                        tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len,\n",
        "                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
        "                )\n",
        "        elif attn_type in [2, 3]: # absolute embeddings\n",
        "            for i in range(n_layer):\n",
        "                self.layers.append(\n",
        "                    DecoderLayer(\n",
        "                        n_head, d_model, d_head, d_inner, dropout,\n",
        "                        dropatt=dropatt, pre_lnorm=pre_lnorm)\n",
        "                )\n",
        "\n",
        "        self.same_length = same_length\n",
        "        self.clamp_len = clamp_len\n",
        "\n",
        "        self._create_params()\n",
        "\n",
        "    def backward_compatible(self):\n",
        "        self.sample_softmax = -1\n",
        "\n",
        "    def _create_params(self):\n",
        "        if self.attn_type == 0: # default attention\n",
        "            self.pos_emb = PositionalEmbedding(self.d_model)\n",
        "            self.r_w_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n",
        "            self.r_r_bias = nn.Parameter(torch.Tensor(self.n_head, self.d_head))\n",
        "        elif self.attn_type == 1: # learnable\n",
        "            self.r_emb = nn.Parameter(torch.Tensor(\n",
        "                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n",
        "            self.r_w_bias = nn.Parameter(torch.Tensor(\n",
        "                    self.n_layer, self.n_head, self.d_head))\n",
        "            self.r_bias = nn.Parameter(torch.Tensor(\n",
        "                    self.n_layer, self.max_klen, self.n_head))\n",
        "        elif self.attn_type == 2: # absolute standard\n",
        "            self.pos_emb = PositionalEmbedding(self.d_model)\n",
        "        elif self.attn_type == 3: # absolute deeper SA\n",
        "            self.r_emb = nn.Parameter(torch.Tensor(\n",
        "                    self.n_layer, self.max_klen, self.n_head, self.d_head))\n",
        "\n",
        "    def reset_length(self, tgt_len, ext_len, mem_len):\n",
        "        self.tgt_len = tgt_len\n",
        "        self.mem_len = mem_len\n",
        "        self.ext_len = ext_len\n",
        "\n",
        "    def init_mems(self, batchsize=None):\n",
        "        if self.mem_len > 0:\n",
        "            mems = []\n",
        "            param = next(self.parameters())\n",
        "            for i in range(self.n_layer+1):\n",
        "                if batchsize is None:\n",
        "                    empty = torch.empty(0, dtype=param.dtype, device=param.device)\n",
        "                else:\n",
        "                    empty = torch.empty(0, batchsize, self.d_model, dtype=param.dtype, device=param.device)\n",
        "                mems.append(empty)\n",
        "\n",
        "            return mems\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _update_mems(self, hids, mems, qlen, mlen, dec_seg_len=None):\n",
        "        # does not deal with None\n",
        "        if mems is None: return None\n",
        "\n",
        "        # mems is not None\n",
        "        assert len(hids) == len(mems), 'len(hids) != len(mems)'\n",
        "\n",
        "        # There are `mlen + qlen` steps that can be cached into mems\n",
        "        # For the next step, the last `ext_len` of the `qlen` tokens\n",
        "        # will be used as the extended context. Hence, we only cache\n",
        "        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n",
        "        # to `mlen + qlen - self.ext_len`.\n",
        "        with torch.no_grad():\n",
        "            new_mems = []\n",
        "\n",
        "            if dec_seg_len is None:\n",
        "                end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n",
        "                beg_idx = max(0, end_idx - self.mem_len)\n",
        "                for i in range(len(hids)):\n",
        "                    cat = torch.cat([mems[i], hids[i]], dim=0)\n",
        "                    new_mems.append(cat[beg_idx:end_idx].detach())\n",
        "\n",
        "            else:      # different len for each sample in batch, `ext_len != 0` is not supported\n",
        "                assert dec_seg_len.size(0) == hids[0].size(1)\n",
        "                batchsize = hids[0].size(1)\n",
        "\n",
        "                for i in range(len(hids)):\n",
        "                    new_layer_mem = []\n",
        "                    for samp_idx in range(batchsize):\n",
        "                        samp_len = dec_seg_len[samp_idx]\n",
        "                        old_samp_mem = mems[i][:, samp_idx, :]\n",
        "                        new_samp_mem = hids[i][:samp_len, samp_idx, :]\n",
        "                        cat = torch.cat([old_samp_mem, new_samp_mem], dim=0)\n",
        "                        end_idx, beg_idx = cat.size(0), max(0, cat.size(0) - self.mem_len)\n",
        "                        new_layer_mem.append(cat[beg_idx:end_idx].detach())\n",
        "\n",
        "                    max_new_mlen = max([cat.size(0) for cat in new_layer_mem])\n",
        "                    for samp_idx in range(batchsize):\n",
        "                        samp_new_mlen = new_layer_mem[ samp_idx ].size(0)\n",
        "                        if samp_new_mlen < max_new_mlen:\n",
        "                            new_layer_mem[samp_idx] = torch.cat([\n",
        "                                torch.zeros(max_new_mlen - samp_new_mlen, mems[i].size(-1), dtype=mems[i].dtype, device=mems[i].device).detach(),\n",
        "                                new_layer_mem[samp_idx]\n",
        "                            ], dim=0)\n",
        "                    new_mems.append(torch.stack(new_layer_mem, dim=1).detach())\n",
        "\n",
        "        return new_mems\n",
        "\n",
        "    def _forward(self, dec_input, segment_emb, mems=None, dec_seg_len=None,\n",
        "                 cross_latent=None, cross_attn_mask=None,\n",
        "                 dec_cross_pos_emb=None, latent_cross_pos_emb=None, return_avg_attn=False):\n",
        "        qlen, bsz, _ = dec_input.size()\n",
        "        # print ('[debug] reached inner _forward()')\n",
        "\n",
        "        if isinstance(mems, tuple) and len(mems) == 1:\n",
        "            mems = mems[0]\n",
        "            assert len(mems) == self.n_layer + 1\n",
        "\n",
        "        mlen = mems[0].size(0) if mems is not None else 0\n",
        "        klen = mlen + qlen\n",
        "        if self.same_length:\n",
        "            all_ones = dec_input.new_ones(qlen, klen)\n",
        "            mask_len = klen - self.mem_len\n",
        "            if mask_len > 0:\n",
        "                mask_shift_len = qlen - mask_len\n",
        "            else:\n",
        "                mask_shift_len = qlen\n",
        "            dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n",
        "                    + torch.tril(all_ones, -mask_shift_len)).bool()[:, :, None] # -1\n",
        "        else:\n",
        "            dec_attn_mask = torch.triu(\n",
        "                dec_input.new_ones(qlen, klen), diagonal=1+mlen).bool()[:,:,None]\n",
        "\n",
        "        hids = []\n",
        "        if return_avg_attn:\n",
        "            all_layer_avg_attn_probs = []\n",
        "        if self.use_segment_emb:\n",
        "            layer_seg_emb = self.seg_emb_projs(segment_emb, layer=0)\n",
        "        else:\n",
        "            layer_seg_emb = torch.zeros_like(dec_input, device=dec_input.device)\n",
        "\n",
        "        if self.use_cross_attn and cross_latent is not None:\n",
        "            layer_cross_latent = self.drop(\n",
        "                                    self.seg_emb_projs(cross_latent, layer=0)\n",
        "                                 )\n",
        "        else:\n",
        "            layer_cross_latent = None\n",
        "        # print ('[cross pos embs]', dec_cross_pos_emb.mean(), latent_cross_pos_emb.mean())\n",
        "\n",
        "        if self.attn_type == 0: # default\n",
        "            pos_seq = torch.arange(klen-1, -1, -1.0, device=dec_input.device,\n",
        "                                   dtype=dec_input.dtype)\n",
        "            if self.clamp_len > 0:\n",
        "                pos_seq.clamp_(max=self.clamp_len)\n",
        "            pos_emb = self.pos_emb(pos_seq)\n",
        "\n",
        "            core_out = self.drop(dec_input)\n",
        "            # print ('[layer 0] inp: {:.3f} (+/- {:.3f}) | segemb: {:.3f} (+/- {:.3f})'.format(\n",
        "            #     core_out.mean().item(), core_out.std().item(),\n",
        "            #     layer_seg_emb[ layer_seg_emb != 0. ].mean().item(), layer_seg_emb[ layer_seg_emb != 0. ].std().item()\n",
        "            # ))\n",
        "            core_out += self.drop(layer_seg_emb)\n",
        "            pos_emb = self.drop(pos_emb)\n",
        "            hids.append(core_out)\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                # print ('[cross latent]', layer_cross_latent.mean())\n",
        "                mems_i = None if mems is None else mems[i]\n",
        "                if not return_avg_attn:\n",
        "                    core_out = layer(\n",
        "                                    core_out, pos_emb, self.r_w_bias,\n",
        "                                    self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i,\n",
        "                                    cross_latent=layer_cross_latent,\n",
        "                                    dec_cross_pos_emb=dec_cross_pos_emb,\n",
        "                                    latent_cross_pos_emb=latent_cross_pos_emb,\n",
        "                                    cross_attn_mask=cross_attn_mask,\n",
        "                                    return_avg_attn=False\n",
        "                                )\n",
        "                else:\n",
        "                    core_out, layer_avg_attn_prob = layer(\n",
        "                                    core_out, pos_emb, self.r_w_bias,\n",
        "                                    self.r_r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i,\n",
        "                                    cross_latent=layer_cross_latent,\n",
        "                                    dec_cross_pos_emb=dec_cross_pos_emb,\n",
        "                                    latent_cross_pos_emb=latent_cross_pos_emb,\n",
        "                                    cross_attn_mask=cross_attn_mask,\n",
        "                                    return_avg_attn=True\n",
        "                                )\n",
        "                    all_layer_avg_attn_probs.append(layer_avg_attn_prob)\n",
        "                    # print ('[avg attn probs]', all_layer_avg_attn_probs[-1].size())\n",
        "\n",
        "                if i != len(self.layers) - 1 and self.in_attn_cond and self.use_segment_emb:\n",
        "                    layer_seg_emb = self.seg_emb_projs(segment_emb, layer=i+1)\n",
        "                    core_out += self.drop(layer_seg_emb)\n",
        "                    if self.use_cross_attn:\n",
        "                        layer_cross_latent = self.drop(\n",
        "                                        self.seg_emb_projs(cross_latent, layer=i+1)\n",
        "                                    )\n",
        "\n",
        "                hids.append(core_out)\n",
        "                # print ('[layer {}] inp: {:.3f} (+/- {:.3f})'.format(\n",
        "                #     i+1, core_out.mean().item(), core_out.std().item()\n",
        "                # ))\n",
        "                # print ('[layer {}] inp: {:.3f} (+/- {:.3f}) | segemb: {:.3f} (+/- {:.3f})'.format(\n",
        "                #     i + 1,\n",
        "                #     core_out.mean().item(), core_out.std().item(),\n",
        "                #     layer_seg_emb[ layer_seg_emb != 0. ].mean().item(), layer_seg_emb[ layer_seg_emb != 0. ].std().item()\n",
        "                # ))\n",
        "\n",
        "        elif self.attn_type == 1: # learnable\n",
        "            core_out = self.drop(dec_input)\n",
        "            core_out += self.drop(layer_seg_emb)\n",
        "            hids.append(core_out)\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if self.clamp_len > 0:\n",
        "                    r_emb = self.r_emb[i][-self.clamp_len :]\n",
        "                    r_bias = self.r_bias[i][-self.clamp_len :]\n",
        "                else:\n",
        "                    r_emb, r_bias = self.r_emb[i], self.r_bias[i]\n",
        "                mems_i = None if mems is None else mems[i]\n",
        "                core_out = layer(core_out, r_emb, self.r_w_bias[i],\n",
        "                        r_bias, dec_attn_mask=dec_attn_mask, mems=mems_i)\n",
        "                if i != len(self.layers) - 1 and self.in_attn_cond and self.use_segment_emb:\n",
        "                    layer_seg_emb = self.seg_emb_projs(segment_emb, layer=i+1)\n",
        "                    core_out += self.drop(layer_seg_emb)\n",
        "                hids.append(core_out)\n",
        "\n",
        "        elif self.attn_type == 2: # absolute\n",
        "            pos_seq = torch.arange(klen - 1, -1, -1.0, device=dec_input.device,\n",
        "                                   dtype=dec_input.dtype)\n",
        "            if self.clamp_len > 0:\n",
        "                pos_seq.clamp_(max=self.clamp_len)\n",
        "            pos_emb = self.pos_emb(pos_seq)\n",
        "\n",
        "            core_out = self.drop(dec_input + pos_emb[-qlen:])\n",
        "            core_out += self.drop(layer_seg_emb)\n",
        "            hids.append(core_out)\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                layer_seg_emb = self.seg_emb_projs(segment_emb, layer=i)\n",
        "                mems_i = None if mems is None else mems[i]\n",
        "                if mems_i is not None and i == 0:\n",
        "                    mems_i += pos_emb[:mlen]\n",
        "                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n",
        "                                 mems=mems_i)\n",
        "                if i != len(self.layers) - 1 and self.in_attn_cond and self.use_segment_emb:\n",
        "                    print ('shouldn\\'t be here !!!')\n",
        "                    layer_seg_emb = self.seg_emb_projs(segment_emb, layer=i+1)\n",
        "                    core_out += self.drop(layer_seg_emb)\n",
        "                hids.append(core_out)\n",
        "\n",
        "        elif self.attn_type == 3:\n",
        "            core_out = self.drop(dec_input)\n",
        "            core_out += self.drop(layer_seg_emb)\n",
        "            hids.append(core_out)\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                mems_i = None if mems is None else mems[i]\n",
        "                if mems_i is not None and mlen > 0:\n",
        "                    cur_emb = self.r_emb[i][:-qlen]\n",
        "                    cur_size = cur_emb.size(0)\n",
        "                    if cur_size < mlen:\n",
        "                        cur_emb_pad = cur_emb[0:1].expand(mlen-cur_size, -1, -1)\n",
        "                        cur_emb = torch.cat([cur_emb_pad, cur_emb], 0)\n",
        "                    else:\n",
        "                        cur_emb = cur_emb[-mlen:]\n",
        "                    mems_i += cur_emb.view(mlen, 1, -1)\n",
        "                core_out += self.r_emb[i][-qlen:].view(qlen, 1, -1)\n",
        "\n",
        "                core_out = layer(core_out, dec_attn_mask=dec_attn_mask,\n",
        "                                 mems=mems_i)\n",
        "                if i != len(self.layers) - 1 and self.in_attn_cond and self.use_segment_emb:\n",
        "                    layer_seg_emb = self.seg_emb_projs(segment_emb, layer=i+1)\n",
        "                    core_out += self.drop(layer_seg_emb)\n",
        "                hids.append(core_out)\n",
        "\n",
        "        core_out = self.drop(core_out)\n",
        "\n",
        "        new_mems = self._update_mems(hids, mems, mlen, qlen, dec_seg_len=dec_seg_len)\n",
        "\n",
        "        if not return_avg_attn:\n",
        "            return core_out, new_mems\n",
        "        else:\n",
        "            return core_out, new_mems, all_layer_avg_attn_probs\n",
        "\n",
        "    def forward(self, dec_input, segment_emb, *mems, dec_seg_len=None, cross_latent=None,\n",
        "                cross_attn_mask=None, dec_cross_pos_seq=None, latent_cross_pos_seq=None, return_avg_attn=False):\n",
        "        if not mems: mems = self.init_mems(batchsize=dec_input.size(1) if dec_seg_len is not None else None)\n",
        "\n",
        "        if self.use_cross_attn is True and dec_cross_pos_seq is not None and latent_cross_pos_seq is not None:\n",
        "            dec_cross_pos_emb = self.cross_pos_emb(dec_cross_pos_seq)\n",
        "            latent_cross_pos_emb = self.cross_pos_emb(latent_cross_pos_seq)\n",
        "            # print ('[cross pos embs]', dec_cross_pos_emb.size(), latent_cross_pos_emb.size())\n",
        "        else:\n",
        "            dec_cross_pos_emb = latent_cross_pos_emb = None\n",
        "\n",
        "        if not return_avg_attn:\n",
        "            dec_out, new_mems = self._forward(\n",
        "                                    dec_input, segment_emb, mems=mems,\n",
        "                                    dec_seg_len=dec_seg_len,\n",
        "                                    cross_latent=cross_latent,\n",
        "                                    cross_attn_mask=cross_attn_mask,\n",
        "                                    dec_cross_pos_emb=dec_cross_pos_emb,\n",
        "                                    latent_cross_pos_emb=latent_cross_pos_emb,\n",
        "                                    return_avg_attn=False\n",
        "                                )\n",
        "        else:\n",
        "            dec_out, new_mems, avg_attn_probs = self._forward(\n",
        "                                                    dec_input, segment_emb, mems=mems,\n",
        "                                                    dec_seg_len=dec_seg_len,\n",
        "                                                    cross_latent=cross_latent,\n",
        "                                                    cross_attn_mask=cross_attn_mask,\n",
        "                                                    dec_cross_pos_emb=dec_cross_pos_emb,\n",
        "                                                    latent_cross_pos_emb=latent_cross_pos_emb,\n",
        "                                                    return_avg_attn=True\n",
        "                                                )\n",
        "\n",
        "        if new_mems is None and not return_avg_attn:\n",
        "            return [dec_out]\n",
        "        elif new_mems is not None and not return_avg_attn:\n",
        "            return [dec_out] + new_mems\n",
        "        else:\n",
        "            return [dec_out] + new_mems, avg_attn_probs\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = 'cpu'\n",
        "\n",
        "    tgt_len, mem_len, ext_len = 128, 600, 0\n",
        "\n",
        "    model = OptimusTXLDecoder(n_layer=12, n_head=8, d_segment_emb=64,\n",
        "                    d_model=512, d_head=64, d_inner=2048,\n",
        "                    dropout=0.1, dropatt=0.1, pre_lnorm=True, tie_seg_emb_projs=False,\n",
        "                    tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len).to(device)\n",
        "\n",
        "    print(sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "    mems = tuple()\n",
        "    for idx in range(10):\n",
        "        # inp = torch.randint(0, 100, (tgt_len, 1))\n",
        "        inp = torch.randn(128, 4, 512)\n",
        "        segment_emb = torch.randn(128, 4, 64)\n",
        "        print('batch {}'.format(idx))\n",
        "        out = model(inp, segment_emb, *mems)\n",
        "        mems = out[1:]\n",
        "        print ('[dec out]', out[0].size())\n",
        "        print ('[mem layer 0]', mems[0].size())"
      ],
      "metadata": {
        "id": "-50WKHC0p96N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "y-wrqjykp_os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformer architecure\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from .optimus_txl_decoder import OptimusTXLDecoder\n",
        "\n",
        "# from .transformer_helpers import (\n",
        "#   WordEmbedding,\n",
        "#   weights_init\n",
        "# )\n",
        "\n",
        "class PlainTransformer(nn.Module):\n",
        "  def __init__(self, d_word_embed, vocab_size,\n",
        "               dec_n_layer, dec_n_head, dec_d_model, dec_d_ff, dec_mem_len, dec_tgt_len,\n",
        "               dec_dropout=0.1, dec_activation='relu',\n",
        "               pad_index=None, pre_lnorm=False,\n",
        "  ):\n",
        "    super(PlainTransformer, self).__init__()\n",
        "\n",
        "    self.d_word_embed = d_word_embed\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    self.dec_n_layer = dec_n_layer\n",
        "    self.dec_n_head = dec_n_head\n",
        "    self.dec_d_model = dec_d_model\n",
        "    self.dec_d_ff = dec_d_ff\n",
        "    self.dec_dropout = dec_dropout\n",
        "    self.dec_activation = dec_activation\n",
        "    self.dec_mem_len = dec_mem_len\n",
        "    self.dec_tgt_len = dec_tgt_len\n",
        "\n",
        "    self.word_emb = WordEmbedding(vocab_size, d_word_embed, dec_d_model)\n",
        "    self.emb_dropout = nn.Dropout(dec_dropout)\n",
        "    if pad_index is None:\n",
        "      self.pad_index = self.vocab_size - 1\n",
        "    else:\n",
        "      self.pad_index = pad_index\n",
        "\n",
        "    self.decoder = OptimusTXLDecoder(\n",
        "                    dec_n_layer, dec_n_head, dec_d_model, dec_d_model // dec_n_head, dec_d_ff,\n",
        "                    None, dec_dropout, dec_dropout,\n",
        "                    tgt_len=dec_tgt_len, mem_len=dec_mem_len, ext_len=0,\n",
        "                    pre_lnorm=pre_lnorm, use_segment_emb=False\n",
        "                  )\n",
        "    self.dec_out_proj = nn.Linear(dec_d_model, vocab_size)\n",
        "\n",
        "    self.apply(weights_init)\n",
        "\n",
        "  def generate(self, dec_input, dec_mems):\n",
        "    dec_word_emb = self.word_emb(dec_input)\n",
        "    dec_input = self.emb_dropout(dec_word_emb)\n",
        "    dec_out = self.decoder(dec_input, None, *dec_mems)\n",
        "    dec_logits = self.dec_out_proj(dec_out[0])[-1, 0, :]\n",
        "    new_dec_mems = dec_out[1:]\n",
        "\n",
        "    return dec_logits, new_dec_mems\n",
        "\n",
        "  def forward(self, dec_input, dec_mems, dec_seg_len=None, return_avg_attn=False):\n",
        "    dec_word_emb = self.word_emb(dec_input)\n",
        "    dec_input = self.emb_dropout(dec_word_emb)\n",
        "    # print ('[debug] in model forward()')\n",
        "\n",
        "    if not return_avg_attn:\n",
        "      dec_out = self.decoder(dec_input, None, *dec_mems, dec_seg_len=dec_seg_len)\n",
        "      dec_logits = self.dec_out_proj(dec_out[0])\n",
        "      new_dec_mems = dec_out[1:]\n",
        "      return dec_logits, new_dec_mems\n",
        "\n",
        "    else:\n",
        "      dec_out, avg_attn_probs = self.decoder(\n",
        "                                  dec_input, None, *dec_mems,\n",
        "                                  dec_seg_len=dec_seg_len,\n",
        "                                  return_avg_attn=True\n",
        "                                )\n",
        "      dec_logits = self.dec_out_proj(dec_out[0])\n",
        "      new_dec_mems = dec_out[1:]\n",
        "\n",
        "      return dec_logits, new_dec_mems, avg_attn_probs\n",
        "\n",
        "  def compute_loss(self, dec_logits, dec_tgt, reduction='mean'):\n",
        "    ce_loss = F.cross_entropy(\n",
        "                    dec_logits.view(-1, dec_logits.size(-1)),\n",
        "                    dec_tgt.contiguous().view(-1),\n",
        "                    ignore_index=self.pad_index,\n",
        "                    reduction=reduction\n",
        "                  )\n",
        "\n",
        "    return {\n",
        "      'ce_loss': ce_loss,\n",
        "      'total_loss': ce_loss\n",
        "    }"
      ],
      "metadata": {
        "id": "7pe6JekEpIEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Melody"
      ],
      "metadata": {
        "id": "c1fJQHsDqnZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "# sys.path.append('./model/')\n",
        "sys.path.append('./')\n",
        "\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# from plain_transformer import PlainTransformer\n",
        "from convert2midi import skyline_event_to_midi, TempoEvent\n",
        "from utils import pickle_load\n",
        "from inference_utils import generate_plain_xl\n",
        "\n",
        "config_path = sys.argv[1]\n",
        "out_dir = sys.argv[2]\n",
        "n_pieces = int(sys.argv[3]) if len(sys.argv) > 3 else 20\n",
        "\n",
        "config = yaml.load(open(config_path, 'r'), Loader=yaml.FullLoader)\n",
        "ckpt_dir = config['output']['ckpt_dir']\n",
        "\n",
        "top_p = 0.97\n",
        "max_dec_len = 2400\n",
        "print ('[nucleus parameters] t = {}, p = {}'.format(temp, top_p))\n",
        "\n",
        "torch.cuda.device(config['device'])\n",
        "\n",
        "# for generation w/ melody prompts\n",
        "use_prompt = False\n",
        "prompt_bars = 8\n",
        "\n",
        "\n",
        "def read_vocab(vocab_file):\n",
        "  event2idx, idx2event = pickle_load(vocab_file)\n",
        "  orig_vocab_size = len(event2idx)\n",
        "  pad_token = orig_vocab_size\n",
        "  event2idx['PAD_None'] = pad_token\n",
        "  vocab_size = pad_token + 1\n",
        "\n",
        "  return event2idx, idx2event, vocab_size\n",
        "\n",
        "\n",
        "def dump_midi(words, idx2event, output_midi_path=None,\n",
        "              rfreq_cls=None, polyph_cls=None, output_event_path=None,\n",
        "              return_tempo=False, enforce_tempo_val=None):\n",
        "  events = [idx2event[w] for w in words]\n",
        "\n",
        "  if output_event_path is not None:\n",
        "    f = open(output_event_path, 'w')\n",
        "    if rfreq_cls is not None:\n",
        "      f.write('[rhymfreq] ')\n",
        "      f.write(str(rfreq_cls))\n",
        "      f.write('\\n')\n",
        "    if polyph_cls is not None:\n",
        "      f.write('[polyph  ] ')\n",
        "      f.write(str(polyph_cls))\n",
        "      f.write('\\n')\n",
        "      f.write('======================================================================\\n')\n",
        "    print (*events, sep='\\n', file=f)\n",
        "\n",
        "  if return_tempo:\n",
        "    return skyline_event_to_midi(events, output_midi_path=output_midi_path, return_tempo=True)[1]\n",
        "  elif enforce_tempo_val is not None:\n",
        "    skyline_event_to_midi(events, output_midi_path=output_midi_path, enforce_tempo=True, enforce_tempo_val=enforce_tempo_val)\n",
        "  else:\n",
        "    skyline_event_to_midi(events, output_midi_path=output_midi_path)\n",
        "\n",
        "\n",
        "def get_leadsheet_prompt(data_dir, piece, prompt_n_bars):\n",
        "  bar_pos, evs = pickle_load(\n",
        "    os.path.join(data_dir, piece + '.pkl')\n",
        "  )\n",
        "\n",
        "  prompt_evs = [\n",
        "    '{}_{}'.format(x['name'], x['value']) for x in evs[ : bar_pos[prompt_n_bars] + 1 ]\n",
        "  ]\n",
        "  assert len( np.where( np.array(prompt_evs) == 'Bar_None' )[0] ) == prompt_n_bars + 1\n",
        "  target_bars = len(bar_pos)\n",
        "\n",
        "  return prompt_evs, target_bars\n",
        "\n",
        "def user_input():\n",
        "    max_bars = int(input('Please input max bars: '))\n",
        "    temp = float(input('Please input temperature (randomness): '))\n",
        "    return (max_bars, temp)"
      ],
      "metadata": {
        "id": "uyvL52ftqpif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "  max_bars, temp = user_input()\n",
        "  event2idx, idx2event, vocab_size = \\\n",
        "    read_vocab(config['data']['vocab_path'])\n",
        "\n",
        "  if use_prompt:\n",
        "    prompt_pieces = pickle_load(config['data']['val_split'])\n",
        "    prompt_pieces = [x for x in prompt_pieces if os.path.exists(\n",
        "      os.path.join(config['data']['data_dir'], x + '.pkl')\n",
        "    )]\n",
        "    if len(prompt_pieces) > n_pieces:\n",
        "      prompt_pieces = random.sample(prompt_pieces, n_pieces)\n",
        "\n",
        "    pickle.dump(\n",
        "        prompt_pieces,\n",
        "        open(os.path.join(out_dir, 'sampled_pieces.pkl'), 'wb')\n",
        "      )\n",
        "    prompts = []\n",
        "    for p in prompt_pieces:\n",
        "      prompts.append(\n",
        "        get_leadsheet_prompt(\n",
        "          config['data']['data_dir'], p,\n",
        "          prompt_bars\n",
        "        )\n",
        "      )\n",
        "\n",
        "\n",
        "  mconf = config['model']\n",
        "  model = PlainTransformer(\n",
        "            mconf['d_word_embed'],\n",
        "            vocab_size,\n",
        "            mconf['decoder']['n_layer'],\n",
        "            mconf['decoder']['n_head'],\n",
        "            mconf['decoder']['d_model'],\n",
        "            mconf['decoder']['d_ff'],\n",
        "            mconf['decoder']['tgt_len'],\n",
        "            mconf['decoder']['tgt_len'],\n",
        "            dec_dropout=mconf['decoder']['dropout'],\n",
        "            pre_lnorm=mconf['pre_lnorm']\n",
        "          ).cuda()\n",
        "  print ('[info] # params:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "  pretrained_dict = torch.load(config['inference_param_path'], map_location='cpu')\n",
        "  model.load_state_dict( pretrained_dict )\n",
        "  model.eval()\n",
        "\n",
        "  generated_pieces = 0\n",
        "  total_pieces = n_pieces\n",
        "  gen_times = []\n",
        "\n",
        "  while generated_pieces < n_pieces:\n",
        "    piece_id = generated_pieces + 1\n",
        "\n",
        "    out_name = 'samp_{:02d}'.format(piece_id)\n",
        "    if os.path.exists(os.path.join(out_dir, out_name + '.mid')):\n",
        "      print ('[info] {} exists, skipping ...'.format(out_name))\n",
        "      continue\n",
        "\n",
        "    if not use_prompt:\n",
        "      tempo_range = range(65, 165, 3)\n",
        "      tempo = random.choice(\n",
        "        tempo_range\n",
        "      )\n",
        "      orig_tempos = [\n",
        "        TempoEvent(tempo, 0, 0)\n",
        "      ]\n",
        "      print ('[global tempo]', orig_tempos[0].tempo)\n",
        "    else:\n",
        "      target_bars = prompts[p][1]\n",
        "      orig_tempos = [\n",
        "        TempoEvent(int(prompts[p][0][0].split('_')[-1]), 0, 0)\n",
        "      ]\n",
        "\n",
        "    print (' -- generating leadsheet #{} of {}'.format(\n",
        "      generated_pieces + 1, total_pieces\n",
        "    ))\n",
        "\n",
        "\n",
        "    if not use_prompt:\n",
        "      gen_words, t_sec = generate_plain_xl(\n",
        "                            model,\n",
        "                            event2idx, idx2event,\n",
        "                            max_events=max_dec_len, max_bars=max_bars,\n",
        "                            primer=['Tempo_{}'.format(orig_tempos[0].tempo), 'Bar_None'],\n",
        "                            temp=temp, top_p=top_p\n",
        "                          )\n",
        "    else:\n",
        "      gen_words, t_sec = generate_plain_xl(\n",
        "                            model,\n",
        "                            event2idx, idx2event,\n",
        "                            max_events=max_dec_len, max_bars=target_bars,\n",
        "                            primer=prompts[p][0],\n",
        "                            temp=temp, top_p=top_p,\n",
        "                            prompt_bars=prompt_bars\n",
        "                          )\n",
        "\n",
        "    if gen_words is None: # model failed repeatedly\n",
        "      continue\n",
        "    if len(gen_words) >= max_dec_len:\n",
        "      continue\n",
        "    if len( np.where( np.array(gen_words) == event2idx[ 'Bar_None' ] )[0] ) >= max_bars:\n",
        "      continue\n",
        "\n",
        "    dump_midi(\n",
        "      gen_words, idx2event,\n",
        "      os.path.join(out_dir, out_name + '.mid'),\n",
        "      output_event_path=os.path.join(out_dir, out_name + '.txt'),\n",
        "      enforce_tempo_val=orig_tempos\n",
        "    )\n",
        "\n",
        "    gen_times.append(t_sec)\n",
        "    generated_pieces += 1\n",
        "\n",
        "  print ('[info] finished generating {} pieces, avg. time: {:.2f} +/- {:.2f} secs.'.format(\n",
        "    generated_pieces, np.mean(gen_times), np.std(gen_times)\n",
        "  ))"
      ],
      "metadata": {
        "id": "VPeXLbUqrB70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fucXk-daquwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l3A98QLLqu1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D5LrfiqGqu3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r0jLkzypqu5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HcnYqS7nqu74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vzead3Cfqu-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compose\n",
        "\n",
        "# Generating a leadsheet\n",
        "!python3 stage01_compose/inference.py \\\n",
        "  stage01_compose/config/pop1k7_finetune.yaml \\\n",
        "  generation/stage01 \\\n",
        "  1   # Generate one leadsheet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVCUhqJmPhLc",
        "outputId": "c3d30a16-503d-464c-a123-f247c1247d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nucleus parameters] t = 1.2, p = 0.97\n",
            "[info] # params: 41331059\n",
            "[global tempo] 143\n",
            " -- generating leadsheet #1 of 1\n",
            "[info] generated 1 bars, #events = 16\n",
            "[info] generated 2 bars, #events = 37\n",
            "[info] generated 3 bars, #events = 59\n",
            "[info] generated 4 bars, #events = 66\n",
            "[info] generated 5 bars, #events = 72\n",
            "[info] generated 6 bars, #events = 91\n",
            "[info] generated 7 bars, #events = 108\n",
            "[info] generated 8 bars, #events = 127\n",
            "[info] generated 9 bars, #events = 145\n",
            "[info] generated 10 bars, #events = 166\n",
            "[info] generated 11 bars, #events = 184\n",
            "[info] generated 12 bars, #events = 195\n",
            "[info] generated 13 bars, #events = 205\n",
            "[info] generated 14 bars, #events = 224\n",
            "[info] generated 15 bars, #events = 243\n",
            "[info] generated 16 bars, #events = 262\n",
            "[info] generated 17 bars, #events = 279\n",
            "[info] generated 18 bars, #events = 303\n",
            "[info] generated 19 bars, #events = 322\n",
            "[info] generated 20 bars, #events = 345\n",
            "[info] generated 21 bars, #events = 371\n",
            "[info] generated 22 bars, #events = 391\n",
            "[info] generated 23 bars, #events = 410\n",
            "[info] generated 24 bars, #events = 434\n",
            "[info] generated 25 bars, #events = 452\n",
            "[info] generated 26 bars, #events = 472\n",
            "[info] generated 27 bars, #events = 490\n",
            "[info] generated 28 bars, #events = 509\n",
            "[info] generated 29 bars, #events = 530\n",
            "[info] generated 30 bars, #events = 548\n",
            "[info] generated 31 bars, #events = 566\n",
            "[info] generated 32 bars, #events = 585\n",
            "[info] generated 33 bars, #events = 606\n",
            "[info] generated 34 bars, #events = 624\n",
            "[info] generated 35 bars, #events = 642\n",
            "[info] generated 36 bars, #events = 660\n",
            "[info] generated 37 bars, #events = 679\n",
            "[info] generated 38 bars, #events = 699\n",
            "[info] generated 39 bars, #events = 715\n",
            "[info] generated 40 bars, #events = 736\n",
            "[info] generated 41 bars, #events = 754\n",
            "[info] generated 42 bars, #events = 762\n",
            "[info] generated 43 bars, #events = 783\n",
            "[info] generated 44 bars, #events = 794\n",
            "[info] generated 45 bars, #events = 799\n",
            "[info] generated 46 bars, #events = 808\n",
            "[info] generated 47 bars, #events = 815\n",
            "[info] generated 48 bars, #events = 820\n",
            "[info] generated 49 bars, #events = 823\n",
            "[info] generated 50 bars, #events = 828\n",
            "[info] generated 51 bars, #events = 835\n",
            "[info] generated 52 bars, #events = 838\n",
            "[info] generated 53 bars, #events = 843\n",
            "[info] generated 54 bars, #events = 848\n",
            "[info] generated 55 bars, #events = 855\n",
            "[info] generated 56 bars, #events = 862\n",
            "[info] generated 57 bars, #events = 867\n",
            "[info] generated 58 bars, #events = 877\n",
            "[info] generated 59 bars, #events = 897\n",
            "[info] generated 60 bars, #events = 916\n",
            "[info] generated 61 bars, #events = 939\n",
            "[info] generated 62 bars, #events = 963\n",
            "[info] generated 63 bars, #events = 981\n",
            "[info] generated 64 bars, #events = 1000\n",
            "[info] generated 65 bars, #events = 1024\n",
            "[info] generated 66 bars, #events = 1042\n",
            "[info] generated 67 bars, #events = 1062\n",
            "[info] generated 68 bars, #events = 1080\n",
            "[info] generated 69 bars, #events = 1099\n",
            "[info] generated 70 bars, #events = 1120\n",
            "[info] generated 71 bars, #events = 1138\n",
            "[info] generated 72 bars, #events = 1156\n",
            "[info] generated 73 bars, #events = 1175\n",
            "[info] generated 74 bars, #events = 1196\n",
            "[info] generated 75 bars, #events = 1214\n",
            "[info] generated 76 bars, #events = 1232\n",
            "[info] generated 77 bars, #events = 1250\n",
            "[info] generated 78 bars, #events = 1269\n",
            "[info] generated 79 bars, #events = 1289\n",
            "[info] generated 80 bars, #events = 1305\n",
            "[info] generated 81 bars, #events = 1326\n",
            "[info] generated 82 bars, #events = 1344\n",
            "[info] generated 83 bars, #events = 1354\n",
            "[info] generated 84 bars, #events = 1359\n",
            "[info] generated 85 bars, #events = 1362\n",
            "[info] generated 86 bars, #events = 1365\n",
            "[info] generated 87 bars, #events = 1368\n",
            "[info] generated 88 bars, #events = 1375\n",
            "[info] generated 89 bars, #events = 1378\n",
            "[info] generated 90 bars, #events = 1383\n",
            "[info] generated 91 bars, #events = 1392\n",
            "[info] generated 92 bars, #events = 1416\n",
            "[info] generated 93 bars, #events = 1434\n",
            "[info] generated 94 bars, #events = 1456\n",
            "[info] generated 95 bars, #events = 1472\n",
            "[info] generated 96 bars, #events = 1494\n",
            "[info] generated 97 bars, #events = 1512\n",
            "[info] generated 98 bars, #events = 1535\n",
            "[info] generated 99 bars, #events = 1550\n",
            "[info] generated 100 bars, #events = 1572\n",
            "[info] generated 101 bars, #events = 1590\n",
            "[info] generated 102 bars, #events = 1612\n",
            "[info] generated 103 bars, #events = 1628\n",
            "[info] generated 104 bars, #events = 1650\n",
            "[info] generated 105 bars, #events = 1668\n",
            "[info] generated 106 bars, #events = 1691\n",
            "[info] gotten eos\n",
            "-- generated events: 1709\n",
            "-- time elapsed: 32.22 secs\n",
            "1 374\n",
            "[[ Start at 0 ], [ End at 0 ], [ Start at 1920 ], [ End at 30720 ], [ Start at 32640 ], [ End at 38400 ], [ Start at 40320 ], [ End at 72960 ], [ Start at 74880 ], [ End at 80640 ], [ Start at 82560 ], [ End at 109440 ], [ Start at 111360 ], [ End at 151680 ], [ Start at 153600 ], [ End at 157440 ], [ Start at 159360 ], [ End at 172800 ], [ Start at 174720 ], [ End at 203520 ]]\n",
            "[info] finished generating 1 pieces, avg. time: 32.22 +/- 0.00 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embellish\n",
        "\n",
        "# We will embellish our generated leadsheet\n",
        "!python3 stage02_embellish/inference.py \\\n",
        "  stage02_embellish/config/pop1k7_default.yaml \\\n",
        "  generation/stage01 \\\n",
        "  generation/stage02"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWy3YTnvP_V_",
        "outputId": "e445d333-c068-46ea-f345-aded1e1f5bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[preparing data] now at #0\n",
            "[info] model init completed\n",
            "[info] model loaded\n",
            "[# pieces] 1\n",
            "/usr/local/lib/python3.10/dist-packages/fast_transformers/feature_maps/fourier_features.py:37: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
            "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
            "Q, R = torch.qr(A, some)\n",
            "should be replaced with\n",
            "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2426.)\n",
            "  Q, _ = torch.qr(block)\n",
            "[info] generated 1 bars, #events = 86\n",
            "[info] generated 2 bars, #events = 220\n",
            "[info] generated 3 bars, #events = 366\n",
            "[info] generated 4 bars, #events = 440\n",
            "[info] generated 5 bars, #events = 522\n",
            "[info] generated 6 bars, #events = 665\n",
            "[info] generated 7 bars, #events = 788\n",
            "[info] generated 8 bars, #events = 910\n",
            "[info] generated 9 bars, #events = 1016\n",
            "[info] generated 10 bars, #events = 1138\n",
            "[info] generated 11 bars, #events = 1259\n",
            "[info] generated 12 bars, #events = 1346\n",
            "[info] generated 13 bars, #events = 1413\n",
            "[info] generated 14 bars, #events = 1523\n",
            "[info] generated 15 bars, #events = 1630\n",
            "[info] generated 16 bars, #events = 1734\n",
            "[info] generated 17 bars, #events = 1807\n",
            "[info] generated 18 bars, #events = 1902\n",
            "[info] generated 19 bars, #events = 2000\n",
            "[info] generated 20 bars, #events = 2120\n",
            "[info] generated 21 bars, #events = 2223\n",
            "[info] generated 22 bars, #events = 2320\n",
            "[info] generated 23 bars, #events = 2409\n",
            "[info] generated 24 bars, #events = 2533\n",
            "[info] generated 25 bars, #events = 2636\n",
            "[info] generated 26 bars, #events = 2729\n",
            "[info] generated 27 bars, #events = 2795\n",
            "[info] generated 28 bars, #events = 2869\n",
            "[info] generated 29 bars, #events = 2948\n",
            "[info] generated 30 bars, #events = 3024\n",
            "[info] generated 31 bars, #events = 3094\n",
            "[info] generated 32 bars, #events = 3165\n",
            "[info] generated 33 bars, #events = 3241\n",
            "[info] generated 34 bars, #events = 3308\n",
            "[info] generated 35 bars, #events = 3375\n",
            "[info] generated 36 bars, #events = 3445\n",
            "[info] generated 37 bars, #events = 3516\n",
            "[info] generated 38 bars, #events = 3594\n",
            "[info] generated 39 bars, #events = 3660\n",
            "[info] generated 40 bars, #events = 3731\n",
            "[info] generated 41 bars, #events = 3801\n",
            "[info] generated 42 bars, #events = 3861\n",
            "[info] generated 43 bars, #events = 3929\n",
            "[info] generated 44 bars, #events = 3981\n",
            "[info] generated 45 bars, #events = 4007\n",
            "[info] generated 46 bars, #events = 4040\n",
            "[info] generated 47 bars, #events = 4062\n",
            "[info] generated 48 bars, #events = 4088\n",
            "[info] generated 49 bars, #events = 4115\n",
            "[info] generated 50 bars, #events = 4144\n",
            "[info] generated 51 bars, #events = 4172\n",
            "[info] generated 52 bars, #events = 4196\n",
            "[info] generated 53 bars, #events = 4222\n",
            "[info] generated 54 bars, #events = 4248\n",
            "[info] generated 55 bars, #events = 4276\n",
            "[info] generated 56 bars, #events = 4308\n",
            "[info] generated 57 bars, #events = 4334\n",
            "[info] generated 58 bars, #events = 4370\n",
            "[info] generated 59 bars, #events = 4425\n",
            "[info] generated 60 bars, #events = 4481\n",
            "[info] generated 61 bars, #events = 4546\n",
            "[info] generated 62 bars, #events = 4610\n",
            "[info] generated 63 bars, #events = 4662\n",
            "[info] generated 64 bars, #events = 4718\n",
            "[info] generated 65 bars, #events = 4785\n",
            "[info] generated 66 bars, #events = 4839\n",
            "[info] generated 67 bars, #events = 4898\n",
            "[info] generated 68 bars, #events = 4952\n",
            "[info] generated 69 bars, #events = 5008\n",
            "[info] generated 70 bars, #events = 5069\n",
            "[info] generated 71 bars, #events = 5133\n",
            "[info] generated 72 bars, #events = 5187\n",
            "[info] generated 73 bars, #events = 5246\n",
            "[info] generated 74 bars, #events = 5307\n",
            "[info] generated 75 bars, #events = 5361\n",
            "[info] generated 76 bars, #events = 5415\n",
            "[info] generated 77 bars, #events = 5479\n",
            "[info] generated 78 bars, #events = 5535\n",
            "[info] generated 79 bars, #events = 5610\n",
            "[info] generated 80 bars, #events = 5673\n",
            "[info] generated 81 bars, #events = 5735\n",
            "[info] generated 82 bars, #events = 5805\n",
            "[info] generated 83 bars, #events = 5859\n",
            "[info] generated 84 bars, #events = 5901\n",
            "[info] generated 85 bars, #events = 5925\n",
            "[info] generated 86 bars, #events = 5943\n",
            "[info] generated 87 bars, #events = 5970\n",
            "[info] generated 88 bars, #events = 5998\n",
            "[info] generated 89 bars, #events = 6025\n",
            "[info] generated 90 bars, #events = 6051\n",
            "[info] generated 91 bars, #events = 6079\n",
            "[info] generated 92 bars, #events = 6139\n",
            "[info] generated 93 bars, #events = 6192\n",
            "[info] generated 94 bars, #events = 6252\n",
            "[info] generated 95 bars, #events = 6296\n",
            "[info] generated 96 bars, #events = 6356\n",
            "[info] generated 97 bars, #events = 6415\n",
            "[info] generated 98 bars, #events = 6478\n",
            "[info] generated 99 bars, #events = 6530\n",
            "[info] generated 100 bars, #events = 6593\n",
            "[info] generated 101 bars, #events = 6652\n",
            "[info] generated 102 bars, #events = 6718\n",
            "[info] generated 103 bars, #events = 6771\n",
            "[info] generated 104 bars, #events = 6838\n",
            "[info] generated 105 bars, #events = 6894\n",
            "[info] generated 106 bars, #events = 6963\n",
            "[info] gotten eos\n",
            "-- generated events: 7013\n",
            "-- time elapsed  : 584.30 secs\n",
            "-- time per event: 0.08 secs\n",
            "# tempo changes: 1 | # notes: 374\n",
            "# tempo changes: 328 | # notes: 1146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installs and imports to convert MIDI into audio\n",
        "!pip install pretty_midi\n",
        "!wget https://www.dropbox.com/s/4x27l49kxcwamp5/GeneralUser_GS_1.471.zip\n",
        "!unzip GeneralUser_GS_1.471.zip\n",
        "!apt install -y fluidsynth\n",
        "from pretty_midi import PrettyMIDI\n",
        "from IPython.display import Audio\n",
        "from scipy.io.wavfile import write\n",
        "import librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlF0MlRJUOpj",
        "outputId": "ecd76d68-10a1-476d-8268-19d4aabf9c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.23.5)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pretty_midi) (1.16.0)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from mido>=1.1.16->pretty_midi) (23.2)\n",
            "--2024-01-21 21:39:07--  https://www.dropbox.com/s/4x27l49kxcwamp5/GeneralUser_G5_1.471.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/4x27l49kxcwamp5/GeneralUser_G5_1.471.zip [following]\n",
            "--2024-01-21 21:39:07--  https://www.dropbox.com/s/raw/4x27l49kxcwamp5/GeneralUser_G5_1.471.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com/cd/0/inline/CLx7uzsqAm39h1G4nKBBF8UcgFfAiYYP3N-9eT9FgE_o93ytGFcUWU5h5zZvC1YjBho6mPW9pV2VeIb1y_-4hVwWrlAt6pcRcCyrky1SfwbW8uERWxQfG_4QOMf2dgDuEts/file# [following]\n",
            "--2024-01-21 21:39:08--  https://uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com/cd/0/inline/CLx7uzsqAm39h1G4nKBBF8UcgFfAiYYP3N-9eT9FgE_o93ytGFcUWU5h5zZvC1YjBho6mPW9pV2VeIb1y_-4hVwWrlAt6pcRcCyrky1SfwbW8uERWxQfG_4QOMf2dgDuEts/file\n",
            "Resolving uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com (uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com (uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CLzflfGhOpVaF7oHwxo6B9d-Xuphc6_mIsjkEz-3kklTL0IvFWjFiXeEF7xM8WoGcaKAF8m1AUrxqhAKJFseYtyBQ3Rr32AtbysISpTgYGXXrwCXntLC4qfDCh4Beq7kYCXeTKJsHfK2dUHaoZ7qDB--98chPTKN_pWrq9X1pFEVYU3wGLPEnwllds0zUvNFZffiStITvpLviSQd9jZ5I8EdYwiUVjhWG97XEbOKQ4zAl-XNAXDaYiyGP9nmsaDnzTLWtjrim_f4nr7c6TlO6ldFTAuTWAF8GN_2tUyw0KZ8Vn0AXXjaaHUtsTWduHQKN3TmJNxgJRiJsHBcnz7Sz5NU30AkrQU-FogTLLqcwsCG8A/file [following]\n",
            "--2024-01-21 21:39:08--  https://uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com/cd/0/inline2/CLzflfGhOpVaF7oHwxo6B9d-Xuphc6_mIsjkEz-3kklTL0IvFWjFiXeEF7xM8WoGcaKAF8m1AUrxqhAKJFseYtyBQ3Rr32AtbysISpTgYGXXrwCXntLC4qfDCh4Beq7kYCXeTKJsHfK2dUHaoZ7qDB--98chPTKN_pWrq9X1pFEVYU3wGLPEnwllds0zUvNFZffiStITvpLviSQd9jZ5I8EdYwiUVjhWG97XEbOKQ4zAl-XNAXDaYiyGP9nmsaDnzTLWtjrim_f4nr7c6TlO6ldFTAuTWAF8GN_2tUyw0KZ8Vn0AXXjaaHUtsTWduHQKN3TmJNxgJRiJsHBcnz7Sz5NU30AkrQU-FogTLLqcwsCG8A/file\n",
            "Reusing existing connection to uc404e5d06cdd4b619aa0029e6ee.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28373045 (27M) [application/zip]\n",
            "Saving to: âGeneralUser_G5_1.471.zip.2â\n",
            "\n",
            "GeneralUser_G5_1.47 100%[===================>]  27.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-01-21 21:39:08 (208 MB/s) - âGeneralUser_G5_1.471.zip.2â saved [28373045/28373045]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fluidsynth is already the newest version (2.2.5-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##########\n",
        "# LISTEN #\n",
        "##########\n",
        "\n",
        "# render the first stage\n",
        "!fluidsynth -ni GeneralUser\\ GS\\ 1.471/GeneralUser\\ GS\\ v1.471.sf2 generation/stage01/samp_01.mid -F first_stage.wav -r 44100\n",
        "\n",
        "# render the second stage\n",
        "!fluidsynth -ni GeneralUser\\ GS\\ 1.471/GeneralUser\\ GS\\ v1.471.sf2 generation/stage02/samp_01_2stage_samp01.mid -F second_stage.wav -r 44100\n",
        "\n",
        "# # uncomment if you want to hear the melody\n",
        "# # generated in the first stage\n",
        "# # hear the first stage\n",
        "# x,sr=librosa.load('first_stage.wav')\n",
        "# Audio(x,rate=sr)\n",
        "\n",
        "# hear the second stage\n",
        "x,sr=librosa.load('second_stage.wav')\n",
        "Audio(x,rate=sr)"
      ],
      "metadata": {
        "id": "V8TrWCyizSfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoqNDTJTwk3F42veX6E5SO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}